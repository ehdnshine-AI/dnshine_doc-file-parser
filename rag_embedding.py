import os
import chromadb
from openai import OpenAI
import glob
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë¡œì»¬ ì €ì¥)
chroma_client = chromadb.PersistentClient(path="./chroma_db")

# ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
collection = chroma_client.get_or_create_collection(
    name="md_documents",
    metadata={"description": "MD íŒŒì¼ ì„ë² ë”© ì»¬ë ‰ì…˜"}
)

def get_openai_embedding(text):
    """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    response = client.embeddings.create(
        model="text-embedding-3-small",  # ë˜ëŠ” "text-embedding-3-large"
        input=text
    )
    return response.data[0].embedding

def split_into_chunks(text, chunk_size=1000, overlap=200):
    """í…ìŠ¤íŠ¸ë¥¼ chunkë¡œ ë¶„í• """
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        
        # ë¹ˆ ì²­í¬ëŠ” ê±´ë„ˆë›°ê¸°
        if chunk.strip():
            chunks.append(chunk)
        
        start = end - overlap
    
    return chunks

def process_md_files(directory_path):
    """MD íŒŒì¼ë“¤ì„ ì½ì–´ì„œ ì„ë² ë”© ìƒì„± ë° ChromaDBì— ì €ì¥"""
    
    md_files = glob.glob(f"{directory_path}/**/*.md", recursive=True)
    
    if not md_files:
        print(f"{directory_path}ì—ì„œ MD íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ì´ {len(md_files)}ê°œì˜ MD íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    
    total_chunks = 0
    
    for idx, file_path in enumerate(md_files):
        print(f"\nì²˜ë¦¬ ì¤‘: {file_path} ({idx+1}/{len(md_files)})")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
            chunks = split_into_chunks(content)
            
            if not chunks:
                print(f"íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            for chunk_idx, chunk in enumerate(chunks):
                # ì„ë² ë”© ìƒì„±
                embedding = get_openai_embedding(chunk)
                
                # ChromaDBì— ì €ì¥
                doc_id = f"{os.path.basename(file_path)}_{chunk_idx}"
                
                collection.add(
                    embeddings=[embedding],
                    documents=[chunk],
                    metadatas=[{
                        "source": file_path,
                        "filename": os.path.basename(file_path),
                        "chunk_index": chunk_idx,
                        "total_chunks": len(chunks)
                    }],
                    ids=[doc_id]
                )
            
            total_chunks += len(chunks)
            print(f"{len(chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    return total_chunks

def query_test(query_text, n_results=3):
    """RAG ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = get_openai_embedding(query_text)
    
    # ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=n_results
    )
    
    return results

def reset_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (ëª¨ë“  ë°ì´í„° ì‚­ì œ)"""
    try:
        chroma_client.delete_collection(name="md_documents")
        print("ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except:
        print("ì´ˆê¸°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("="*60)
    print("RAG ì‹œìŠ¤í…œ - ì„ë² ë”© ìƒì„± ë° ì €ì¥")
    print("="*60)
    
    # ì‚¬ìš©ì ì„ íƒ
    print("\nì„ íƒí•˜ì„¸ìš”:")
    print("1. MD íŒŒì¼ ì„ë² ë”© ìƒì„± ë° ì €ì¥")
    print("2. í…ŒìŠ¤íŠ¸ ê²€ìƒ‰")
    print("3. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”")
    print("4. ì¢…ë£Œ")
    
    choice = input("\nì„ íƒ (1-4): ").strip()
    
    if choice == "1":
        directory = input("MD íŒŒì¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: ./md_files): ").strip()
        if not directory:
            directory = "./output_folder"
        
        print(f"\nğŸ“ ë””ë ‰í† ë¦¬: {directory}")
        total = process_md_files(directory)
        
        print("\n" + "="*60)
        print(f"âœ… ì €ì¥ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ {collection.count()}ê°œì˜ ì²­í¬ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("="*60)
    
    elif choice == "2":
        query = input("\nê²€ìƒ‰í•  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if query:
            print("\nğŸ” ê²€ìƒ‰ ì¤‘...\n")
            results = query_test(query, n_results=3)
            
            print(f"ì§ˆë¬¸: {query}\n")
            print("ê²€ìƒ‰ ê²°ê³¼:")
            print("-"*60)
            
            for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):
                print(f"\n[ê²°ê³¼ {i+1}]")
                print(f"ğŸ“„ ì¶œì²˜: {metadata['filename']}")
                print(f"ğŸ“ ì²­í¬: {metadata['chunk_index'] + 1}/{metadata['total_chunks']}")
                print(f"ğŸ“ ë‚´ìš©: {doc[:300]}...")
                print("-"*60)
    
    elif choice == "3":
        confirm = input("ì •ë§ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): ").strip().lower()
        if confirm == "yes":
            reset_database()
    
    elif choice == "4":
        print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
    
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")